{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python","version":"3.x"},"widgets":{"state":{},"application/vnd.jupyter.widget-state+json":{"state":{},"version_major":2,"version_minor":0}}},"cells":[{"cell_type":"code","metadata":{},"execution_count":null,"outputs":[],"source":["# @title 1. Install Dependencies (Run First)\n","!pip install -q gensim spacy torch transformers diffusers gtts pillow accelerate scipy\n","!python -m spacy download en_core_web_md\n","print(\"✅ Libraries installed successfully.\")\n","\n","\n","\n","# @title 2. Run Embeddings Tasks (GloVe, spaCy, FastText)\n","import gensim.downloader as api\n","import spacy\n","from gensim.models import FastText\n","import numpy as np\n","\n","# --- Load Models ---\n","print(\"⏳ Loading GloVe model (approx 1 min)...\")\n","glove = api.load(\"glove-wiki-gigaword-50\")\n","nlp = spacy.load(\"en_core_web_md\")\n","print(\"✅ Models Loaded.\")\n","\n","# ---------------------------------------------------------\n","# TASK 1: Skill Relationships (GloVe)\n","# ---------------------------------------------------------\n","print(\"\n--- TASK 1: Skill Relationships ---\")\n","skill = \"python\"\n","# 1. Top 5 Similar Skills\n","if skill in glove:\n","    sims = glove.most_similar(skill, topn=5)\n","    print(f\"Skills similar to '{skill}': {sims}\")\n","\n","    # 2. Odd one out\n","    candidates = [\"python\", \"java\", \"c\", \"html\", \"ruby\"] # used 'c' as c++ often tokenized differently\n","    candidates_clean = [w for w in candidates if w in glove]\n","    odd = glove.doesnt_match(candidates_clean)\n","    print(f\"Odd one out in {candidates}: {odd}\")\n","else:\n","    print(f\"Skill {skill} not in vocabulary.\")\n","\n","# ---------------------------------------------------------\n","# TASK 2: Headline Similarity (spaCy)\n","# ---------------------------------------------------------\n","print(\"\n--- TASK 2: Headline Similarity ---\")\n","h1 = \"Stocks crash as inflation rises\"\n","h2 = \"Market falls due to high prices\"\n","doc1 = nlp(h1)\n","doc2 = nlp(h2)\n","print(f\"Headline 1: {h1}\nHeadline 2: {h2}\nSimilarity Score: {doc1.similarity(doc2):.4f}\")\n","\n","# Keyword similarity\n","w1, w2 = nlp(\"economy\") [0], nlp(\"finance\") [0]\n","print(f\"Similarity between 'economy' and 'finance': {w1.similarity(w2):.4f}\")\n","\n","# ---------------------------------------------------------\n","# TASK 3: Misspelled Words (FastText)\n","# ---------------------------------------------------------\n","print(\"\n--- TASK 3: FastText for Misspellings ---\")\n","# Training a small FastText model on dummy data\n","sentences = [[\"great\", \"product\", \"fast\", \"delivery\"],\n","             [\"bad\", \"service\", \"slow\", \"shipping\"],\n","             [\"excellent\", \"quality\", \"recommended\"]]\n","ft_model = FastText(sentences, vector_size=10, window=3, min_count=1, epochs=10)\n","\n","misspelled = \"prodct\" # Missing 'u'\n","# FastText generates vectors for unseen words using n-grams\n","if misspelled in ft_model.wv:\n","    print(f\"Vector found for '{misspelled}'? Yes\")\n","    print(f\"Most similar to '{misspelled}': {ft_model.wv.most_similar(misspelled, topn=3)}\")\n","else:\n","    # Fallback if n-grams fail (rare in full models)\n","    print(f\"Could not generate vector for {misspelled}\")\n","\n","# ---------------------------------------------------------\n","# TASK 4: Analogies (GloVe)\n","# ---------------------------------------------------------\n","print(\"\n--- TASK 4: Analogies ---\")\n","# Doctor : Hospital :: Teacher : ?\n","# Vector math: Teacher + (Hospital - Doctor)\n","try:\n","    analogy = glove.most_similar(positive=[\"teacher\", \"hospital\"], negative=[\"doctor\"], topn=1)\n","    print(f\"Doctor is to Hospital as Teacher is to: {analogy[0][0].upper()}\")\n","except Exception as e:\n","    print(\"Analogy error:\", e)\n","\n","\n","\n","# @title 3. Run Text Generation & Analysis (GPT-2, BERT)\n","from transformers import pipeline, set_seed\n","\n","# --- Load Pipelines ---\n","print(\"⏳ Loading Transformers Pipelines...\")\n","generator = pipeline('text-generation', model='gpt2')\n","sentiment_task = pipeline(\"sentiment-analysis\")\n","summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n","set_seed(42)\n","print(\"✅ Pipelines Loaded.\")\n","\n","# ---------------------------------------------------------\n","# TASK 5 & 11: Content Generation\n","# ---------------------------------------------------------\n","print(\"\n--- TASK 5: Generate Content ---\")\n","# 1. Exam Notification\n","prompt_exam = \"NOTICE: The university final semester exams will commence on\"\n","exam_notif = generator(prompt_exam, max_length=50, num_return_sequences=1)\n","print(f\"Generated Notification:\n{exam_notif[0]['generated_text']}\n\")\n","\n","# 2. Inspirational Message\n","prompt_story = \"To all final year students, remember that success is\"\n","story = generator(prompt_story, max_length=60, num_return_sequences=1)\n","print(f\"Inspirational Message:\n{story[0]['generated_text']}\")\n","\n","# ---------------------------------------------------------\n","# TASK 8 & 12: Sentiment & Feedback\n","# ---------------------------------------------------------\n","print(\"\n--- TASK 8 & 12: Feedback Analysis ---\")\n","feedback = \"The course content was excellent and the labs were helpful, but the duration was too short.\"\n","# 1. Sentiment\n","result = sentiment_task(feedback)[0]\n","print(f\"Feedback: '{feedback}'\")\n","print(f\"Sentiment: {result['label']} (Score: {result['score']:.4f})\")\n","\n","# 2. Summary\n","long_text = \""\n","Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to the natural intelligence\n","displayed by animals including humans. AI research has been defined as the field of study of intelligent agents,\n","which refers to any system that perceives its environment and takes actions that maximize its chance of achieving its goals.\n","\"\n","summary = summarizer(long_text, max_length=40, min_length=10, do_sample=False)\n","print(f\"\nSummary of academic text:\n{summary[0]['summary_text']}\")\n","\n","# ---------------------------------------------------------\n","# TASK 13: Code Generation (Simulated with GPT-2)\n","# ---------------------------------------------------------\n","# Note: For real code gen, use models like 'codegen' or 'copilot'. GPT-2 is weak at code.\n","print(\"\n--- TASK 13: Code Generation Example ---\")\n","code_prompt = \"def calculate_average(numbers):\"\n","code_gen = generator(code_prompt, max_length=50)[0]['generated_text']\n","print(f\"Generated Code Snippet:\n{code_gen}\")\n","\n","\n","\n","# @title 4. Run Multimodal Tasks (Audio, Image, Video)\n","import torch\n","from gtts import gTTS\n","from IPython.display import Audio, display\n","from transformers import BlipProcessor, BlipForConditionalGeneration\n","from diffusers import StableDiffusionPipeline\n","from PIL import Image\n","import requests\n","from io import BytesIO\n","\n","# ---------------------------------------------------------\n","# TASK 7 & 10: Text to Audio (gTTS)\n","# ---------------------------------------------------------\n","print(\"\n--- TASK 7: Text to Audio ---\")\n","text = \"Welcome to the AI content assistant system.\"\n","tts = gTTS(text, lang='en')\n","tts.save('output.mp3')\n","print(\"Audio generated: 'output.mp3'\")\n","display(Audio('output.mp3', autoplay=False))\n","\n","# ---------------------------------------------------------\n","# TASK 6, 9, 10: Image Captioning (BLIP)\n","# ---------------------------------------------------------\n","print(\"\n--- TASK 6: Image Captioning ---\")\n","# Load Model\n","processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n","model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\")\n","\n","# Download sample image\n","img_url = \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg\"\n","raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n","display(raw_image.resize((300, 200))) # Show image\n","\n","# Generate Caption\n","inputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\")\n","out = model.generate(**inputs)\n","caption = processor.decode(out[0], skip_special_tokens=True)\n","print(f\"Generated Caption: {caption}\")\n","\n","# ---------------------------------------------------------\n","# TASK 14: Text-to-Image (Stable Diffusion)\n","# ---------------------------------------------------------\n","print(\"\n--- TASK 14: Text to Image ---\")\n","# Load Pipeline (Requires GPU)\n","pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n","pipe = pipe.to(\"cuda\")\n","\n","prompt = \"Futuristic city with flying cars at sunset, cyberpunk style\"\n","image = pipe(prompt).images[0]\n","print(f\"Prompt: {prompt}\")\n","display(image)\n","\n","# ---------------------------------------------------------\n","# TASK 15: Video Generation (Runway ML Placeholder)\n","# ---------------------------------------------------------\n","print(\"\n--- TASK 15: Video Generation (Note) ---\")\n","print(\"Runway ML requires a paid API key. For this Colab, we use ModelScope (Open Source) for text-to-video if you wish to implement it.\")\n","print(\"To use ModelScope (Text-to-Video), uncomment the lines below (WARNING: Uses high VRAM):\")\n","\n","# from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n","# from google.colab import files\n","# pipe_video = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\")\n","# pipe_video.scheduler = DPMSolverMultistepScheduler.from_config(pipe_video.scheduler.config)\n","# pipe_video.enable_model_cpu_offload()\n","# video_frames = pipe_video(\"A robot dancing in a futuristic city\", num_inference_steps=25).frames\n","# print(\"Video frames generated.\")\n"]}]}